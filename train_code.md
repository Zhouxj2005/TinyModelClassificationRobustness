# train_distill.py

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import argparse
import os
import csv
import time
from models import get_model
from augmix_ops import AugMixDataset

# --- å‚æ•°é…ç½® ---
parser = argparse.ArgumentParser(description='Robust Distillation Training')
parser.add_argument('--student', default='mobilenet_v2', type=str, help='Student model architecture')
parser.add_argument('--teacher', default='resnet18_cifar', type=str, help='Teacher model architecture')
parser.add_argument('--teacher_path', default='./final_checkpoint/resnet18-sota.pth', type=str, help='Path to teacher checkpoint')
parser.add_argument('--epochs', default=200, type=int)
parser.add_argument('--lr', default=0.05, type=float) # è’¸é¦é€šå¸¸å¯ä»¥ç”¨ç¨å¾®å¤§ä¸€ç‚¹çš„LR
parser.add_argument('--batch_size', default=128, type=int)
parser.add_argument('--alpha', default=15.0, type=float, help='Weight for KD Loss')
parser.add_argument('--temperature', default=3.0, type=float, help='Temperature for KD')
parser.add_argument('--save_dir', default='./checkpoint', type=str)
args = parser.parse_args()

# --- æ ¸å¿ƒç»„ä»¶ï¼šKL æ•£åº¦è’¸é¦ Loss ---
def loss_kd(outputs, teacher_outputs, temperature):
    """
    Args:
        outputs: Student çš„ Logits
        teacher_outputs: Teacher çš„ Logits
        temperature: è½¯åŒ–æ¸©åº¦ (Tè¶Šå¤§ï¼Œåˆ†å¸ƒè¶Šå¹³æ»‘ï¼Œå…³æ³¨éä¸»ç±»ä¿¡æ¯)
    """
    T = temperature
    # KLDivLoss æœŸæœ›è¾“å…¥æ˜¯ log_softmaxï¼Œç›®æ ‡æ˜¯ softmax
    loss = nn.KLDivLoss(reduction='batchmean')(
        F.log_softmax(outputs / T, dim=1),
        F.softmax(teacher_outputs / T, dim=1)
    ) * (T * T) # æŒ‰ç…§ Hinton çš„è®ºæ–‡ï¼Œéœ€è¦ä¹˜ T^2 ä¿æŒæ¢¯åº¦é‡çº§
    return loss

def train_distill():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"=== å¼€å§‹é²æ£’è’¸é¦ (Robust Distillation) ===")
    print(f"   Teacher: {args.teacher} | Student: {args.student}")
    print(f"   Alpha: {args.alpha} | Temperature: {args.temperature}")

    # 1. å‡†å¤‡æ•°æ® (å’Œ AugMix è®­ç»ƒä¸€è‡´)
    transform_final = transforms.Compose([transforms.ToTensor()])
    transform_train_base = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip()])
    
    trainset_raw = torchvision.datasets.CIFAR100(root='./data/cifar100', train=True, download=True, transform=transform_train_base)
    trainset = AugMixDataset(trainset_raw, transform_final)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=8, pin_memory=True)

    transform_test = transforms.Compose([transforms.ToTensor()])
    testset = torchvision.datasets.CIFAR100(root='./data/cifar100', train=False, download=True, transform=transform_test)
    testloader = torch.utils.data.DataLoader(testset, batch_size=args.batch_size, shuffle=False, num_workers=8, pin_memory=True)

    # 2. åŠ è½½ Teacher æ¨¡å‹ (å†»ç»“å‚æ•°)
    print(f"=> Loading Teacher from {args.teacher_path}...")
    teacher = get_model(args.teacher, num_classes=100).to(device)
    # åŠ è½½æƒé‡
    try:
        checkpoint = torch.load(args.teacher_path)
        teacher.load_state_dict(checkpoint['net'])
        print(f"   Teacher Accuracy (Recorded): {checkpoint.get('acc', 'N/A')}%")
    except Exception as e:
        print(f"âŒ Error loading teacher: {e}")
        return

    teacher.eval() # âš ï¸ å…³é”®ï¼šTeacher å¿…é¡»å§‹ç»ˆå¤„äº Eval æ¨¡å¼
    # å†»ç»“å‚æ•°ï¼ŒèŠ‚çœæ˜¾å­˜å’Œè®¡ç®—
    for param in teacher.parameters():
        param.requires_grad = False

    # 3. åˆå§‹åŒ– Student æ¨¡å‹
    student = get_model(args.student, num_classes=100).to(device)

    # 4. ä¼˜åŒ–å™¨
    optimizer = optim.SGD(student.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)

    # æ—¥å¿—
    if not os.path.exists(args.save_dir): os.makedirs(args.save_dir)
    log_path = os.path.join(args.save_dir, f'{args.student}_distill_{args.alpha}_{args.epochs}_{args.lr}_log.csv')
    with open(log_path, 'w', newline='') as f:
        csv.writer(f).writerow(['epoch', 'loss', 'clean_acc', 'time'])

    best_acc = 0.0

    # 5. è®­ç»ƒå¾ªç¯
    for epoch in range(args.epochs):
        student.train()
        train_loss = 0.0
        start_time = time.time()
        
        for i, (images_clean, images_aug1, images_aug2, targets) in enumerate(trainloader):
            # Move to GPU
            images_clean, images_aug1, images_aug2, targets = \
                images_clean.to(device), images_aug1.to(device), images_aug2.to(device), targets.to(device)
            
            # æ‹¼æ¥: Batch x 3
            images_all = torch.cat([images_clean, images_aug1, images_aug2], dim=0)

            # --- Forward Pass ---
            optimizer.zero_grad()

            # 1. Student Forward
            logits_all_s = student(images_all)
            logits_clean_s, logits_aug1_s, logits_aug2_s = torch.split(logits_all_s, images_clean.size(0))

            # 2. Teacher Forward (No Grad)
            with torch.no_grad():
                logits_all_t = teacher(images_all)
                # æˆ‘ä»¬ä¸éœ€è¦æ‹†åˆ† Teacher çš„ logitsï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯æ•´ä½“è’¸é¦

            # --- Loss Calculation (ä½ çš„åˆ›æ–°ç»„åˆæ‹³) ---
            
            # Part A: Cross Entropy (Student å¿…é¡»åšå¯¹ Clean æ ·æœ¬çš„åˆ†ç±»)
            loss_ce = F.cross_entropy(logits_clean_s, targets)

            # Part B: AugMix Consistency (Student è‡ªæˆ‘çº¦æŸï¼šä¸åŒè§†è§’è¾“å‡ºè¦ä¸€è‡´)
            p_clean = F.softmax(logits_clean_s, dim=1)
            p_aug1 = F.softmax(logits_aug1_s, dim=1)
            p_aug2 = F.softmax(logits_aug2_s, dim=1)
            p_mixture = torch.clamp((p_clean + p_aug1 + p_aug2) / 3., 1e-7, 1).log()
            loss_js = (F.kl_div(p_mixture, p_clean, reduction='batchmean') +
                       F.kl_div(p_mixture, p_aug1, reduction='batchmean') +
                       F.kl_div(p_mixture, p_aug2, reduction='batchmean')) / 3.

            # Part C: Knowledge Distillation (Teacher çº¦æŸï¼šä½ è¦åƒæˆ‘ä¸€æ ·æ€è€ƒ)
            # å…³é”®ï¼šæˆ‘ä»¬åœ¨æ‰€æœ‰æ ·æœ¬(Clean + Aug)ä¸Šéƒ½è¿›è¡Œè’¸é¦
            loss_kd_val = loss_kd(logits_all_s, logits_all_t, args.temperature)

            # --- Total Loss ---
            # è¿™é‡Œçš„ç³»æ•°æ˜¯å¯ä»¥è°ƒä¼˜çš„è¶…å‚æ•°
            # 1.0 * CE + 12 * JS + 1.0 * KD
            loss = loss_ce + 12 * loss_js + args.alpha * loss_kd_val

            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        # Validation
        student.eval()
        correct = 0; total = 0
        with torch.no_grad():
            for inputs, targets in testloader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = student(inputs)
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        acc = 100.*correct/total
        scheduler.step()

        epoch_time = time.time() - start_time
        avg_loss = train_loss/(i+1)
        print(f"Epoch {epoch+1} | Time: {epoch_time:.1f}s | Loss: {avg_loss:.3f} | loss_ce: {loss_ce} | loss_js: {12*loss_js} | loss_kd_val: {args.alpha * loss_kd_val} | Clean Acc: {acc:.2f}%")
        
        # Log & Save
        with open(log_path, 'a', newline='') as f:
            csv.writer(f).writerow([epoch+1, avg_loss, acc, epoch_time])

        if acc > best_acc:
            best_acc = acc
            torch.save({'net': student.state_dict(), 'acc': acc}, 
                       os.path.join(args.save_dir, f'{args.student}_distill_{args.alpha}_{args.epochs}_{args.lr}.pth'))

    print(f"=== è’¸é¦å®Œæˆ. Best Acc: {best_acc:.2f}% ===")

if __name__ == "__main__":
    train_distill()
```

# train_distill_improved.py

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import argparse
import os
import csv
import time
import math
from models import get_model
from augmix_ops import AugMixDataset

# ==========================================
# 0. å‚æ•°é…ç½®
# ==========================================
parser = argparse.ArgumentParser(description='Improved Robust Distillation Training')
parser.add_argument('--student', default='mobilenet_v2', type=str)
parser.add_argument('--teacher', default='resnet18', type=str)
parser.add_argument('--teacher_path', default='./checkpoint/resnet18_200_0.1_augmix.pth', type=str)
parser.add_argument('--epochs', default=200, type=int)
# MobileNetV2 é…åˆ Warmup å»ºè®®ä½¿ç”¨ 0.1ï¼Œå¦‚æœä¸ç¨³å¯é™è‡³ 0.05
parser.add_argument('--lr', default=0.1, type=float) 
parser.add_argument('--batch_size', default=128, type=int)

# --- è’¸é¦è¶…å‚æ•° ---
parser.add_argument('--alpha', default=20.0, type=float, help='Weight for Soft Target KD')
parser.add_argument('--beta', default=500.0, type=float, help='Weight for RKD (Structure) Loss')
parser.add_argument('--temperature', default=4.0, type=float, help='Temperature for KD')
parser.add_argument('--js_lambda', default=12.0, type=float, help='Weight for AugMix Consistency')

parser.add_argument('--save_dir', default='./checkpoint', type=str)
args = parser.parse_args()

# ==========================================
# 1. å·¥å…·ç±»: FeatureWrapper
#    (æ— éœ€ä¿®æ”¹ models.py å³å¯æå–ç‰¹å¾)
# ==========================================
class FeatureWrapper(nn.Module):
    """
    åŒ…è£…å™¨ï¼šé€šè¿‡ Hook æœºåˆ¶è‡ªåŠ¨æå–å€’æ•°ç¬¬äºŒå±‚çš„ç‰¹å¾ã€‚
    è¿”å›: (logits, features)
    """
    def __init__(self, model):
        super().__init__()
        self.model = model
        self.features = None
        self.hook_handle = None
        self._register_hook()

    def _find_target_layer(self):
        # é€’å½’å¯»æ‰¾çœŸæ­£çš„æ¨¡å‹å®ä½“ï¼ˆå¤„ç† NormalizedModel åŒ…è£…ï¼‰
        real_model = self.model.model if hasattr(self.model, 'model') else self.model
        
        # é’ˆå¯¹ä¸åŒæ¶æ„å¯»æ‰¾â€œæœ€åä¸€å±‚å…¨è¿æ¥å±‚â€
        if hasattr(real_model, 'linear'): 
            return real_model.linear # ResNetRobustBench
        elif hasattr(real_model, 'fc'):
            return real_model.fc # Standard ResNet
        elif hasattr(real_model, 'classifier'):
            # MobileNetV2: classifier æ˜¯ä¸€ä¸ª Sequential
            return real_model.classifier 
        else:
            raise ValueError("æ— æ³•è‡ªåŠ¨å®šä½æœ€åä¸€å±‚ (Linear/FC/Classifier)ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç»“æ„ã€‚")

    def _hook_fn(self, module, input):
        # ğŸŒŸ ä¿®æ­£ç‚¹ï¼špre_hook åªæœ‰ (module, input) ä¸¤ä¸ªå‚æ•°ï¼Œæ²¡æœ‰ output
        # å…¨è¿æ¥å±‚çš„è¾“å…¥å°±æ˜¯æˆ‘ä»¬è¦çš„ç‰¹å¾
        # input æ˜¯ä¸€ä¸ª tupleï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
        feat = input[0]
        # å±•å¹³ç‰¹å¾ [Batch, C, 1, 1] -> [Batch, C]
        self.features = feat.flatten(1)

    def _register_hook(self):
        target_layer = self._find_target_layer()
        # æ³¨å†Œ Forward Pre Hookï¼šåœ¨è¿›å…¥å…¨è¿æ¥å±‚ä¹‹å‰æˆªè·è¾“å…¥
        # è¿™é‡Œçš„è¾“å…¥å°±æ˜¯ Feature
        self.hook_handle = target_layer.register_forward_pre_hook(self._hook_fn)

    def forward(self, x):
        # è¿™ä¸€æ­¥ä¼šè§¦å‘ hookï¼Œæ›´æ–° self.features
        logits = self.model(x)
        return logits, self.features

# ==========================================
# 2. æŸå¤±å‡½æ•° (Loss Functions)
# ==========================================

def rkd_loss(student_feat, teacher_feat):
    """
    Relational Knowledge Distillation (Distance-wise)
    è®© Student å­¦ä¹  Teacher çš„æ ·æœ¬é—´å‡ ä½•è·ç¦»å…³ç³»ã€‚
    """
    # 1. ç‰¹å¾å½’ä¸€åŒ– (æ¶ˆé™¤é‡çº²å·®å¼‚)
    student_feat = F.normalize(student_feat, p=2, dim=1)
    teacher_feat = F.normalize(teacher_feat, p=2, dim=1)

    # 2. è®¡ç®—æˆå¯¹æ¬§æ°è·ç¦»çŸ©é˜µ [Batch, Batch]
    t_dist = torch.cdist(teacher_feat, teacher_feat, p=2)
    s_dist = torch.cdist(student_feat, student_feat, p=2)

    # 3. å½’ä¸€åŒ–è·ç¦»çŸ©é˜µ (é™¤ä»¥çŸ©é˜µå‡å€¼ï¼Œå…³æ³¨ç›¸å¯¹å…³ç³»è€Œéç»å¯¹æ•°å€¼)
    # åŠ ä¸Š epsilon é˜²æ­¢é™¤é›¶
    t_mean = t_dist.mean() + 1e-8
    s_mean = s_dist.mean() + 1e-8
    
    t_dist_norm = t_dist / t_mean
    s_dist_norm = s_dist / s_mean

    # 4. è®¡ç®—çŸ©é˜µå·®å¼‚ (Huber Loss æ¯” MSE æ›´ç¨³å¥)
    loss = F.smooth_l1_loss(s_dist_norm, t_dist_norm)
    return loss

def confidence_weighted_kd_loss(outputs, teacher_outputs, temperature):
    """
    ç½®ä¿¡åº¦åŠ æƒçš„ KD Lossã€‚
    Teacher è¶Šç¡®å®š (Max Prob è¶Šé«˜)ï¼ŒLoss æƒé‡è¶Šå¤§ã€‚
    """
    T = temperature
    
    # 1. è®¡ç®— Teacher çš„ç½®ä¿¡åº¦æƒé‡
    with torch.no_grad():
        t_probs = F.softmax(teacher_outputs, dim=1)
        t_conf, _ = t_probs.max(dim=1) # [Batch]
        # æƒé‡å¯ä»¥ç›´æ¥ç”¨ç½®ä¿¡åº¦ï¼Œä¹Ÿå¯ä»¥åšä¸€ä¸ªéçº¿æ€§æ˜ å°„
        loss_weight = t_conf.detach()

    # 2. è®¡ç®—é€æ ·æœ¬çš„ KL æ•£åº¦
    # reduction='none' ä¿ç•™ [Batch] ç»´åº¦
    loss_pointwise = nn.KLDivLoss(reduction='none')(
        F.log_softmax(outputs / T, dim=1),
        F.softmax(teacher_outputs / T, dim=1)
    ) * (T * T)
    
    # KLDivLoss è¾“å‡ºé€šå¸¸æ˜¯ [Batch, Classes]ï¼Œæ±‚å’Œå¾—åˆ°æ¯ä¸ªæ ·æœ¬çš„ Loss
    loss_sample = loss_pointwise.sum(dim=1)
    
    # 3. åŠ æƒå¹³å‡
    loss = (loss_sample * loss_weight).mean()
    return loss

def get_lr_scheduler(optimizer, total_epochs, warmup_epochs=5):
    """ Warmup + Cosine Scheduler """
    def lr_lambda(epoch):
        if epoch < warmup_epochs:
            return (epoch + 1) / warmup_epochs
        else:
            return 0.5 * (1 + math.cos(math.pi * (epoch - warmup_epochs) / (total_epochs - warmup_epochs)))
    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

# ==========================================
# 3. ä¸»è®­ç»ƒæµç¨‹
# ==========================================
def train_distill_improved():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"=== å¼€å§‹æ”¹è¿›ç‰ˆé²æ£’è’¸é¦ (Improved Robust Distillation) ===")
    print(f"   Teacher: {args.teacher} -> Student: {args.student}")
    print(f"   KD Alpha: {args.alpha} | RKD Beta: {args.beta} | JS Lambda: {args.js_lambda}")

    # --- 1. æ•°æ®å‡†å¤‡ ---
    # è¿™é‡Œçš„ transform åªåš ToTensorï¼Œå½’ä¸€åŒ–ç”±æ¨¡å‹å†…éƒ¨ NormalizedModel å®Œæˆ
    transform_final = transforms.Compose([transforms.ToTensor()])
    transform_train_base = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip()
    ])
    
    trainset_raw = torchvision.datasets.CIFAR100(root='./data/cifar100', train=True, download=True, transform=transform_train_base)
    trainset = AugMixDataset(trainset_raw, transform_final)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=8, pin_memory=True)

    transform_test = transforms.Compose([transforms.ToTensor()])
    testset = torchvision.datasets.CIFAR100(root='./data/cifar100', train=False, download=True, transform=transform_test)
    testloader = torch.utils.data.DataLoader(testset, batch_size=args.batch_size, shuffle=False, num_workers=8, pin_memory=True)

    # --- 2. æ¨¡å‹åŠ è½½ä¸åŒ…è£… ---
    print(f"=> Preparing models...")
    
    # Teacher
    teacher_raw = get_model(args.teacher, num_classes=100).to(device)
    # åŠ è½½æƒé‡é€»è¾‘
    if os.path.exists(args.teacher_path):
        ckpt = torch.load(args.teacher_path, map_location=device)
        # å…¼å®¹å¤„ç†
        if isinstance(ckpt, dict) and 'net' in ckpt: state_dict = ckpt['net']
        elif isinstance(ckpt, dict) and 'state_dict' in ckpt: state_dict = ckpt['state_dict']
        else: state_dict = ckpt
        
        # ç§»é™¤ model. å‰ç¼€ (å¦‚æœæœ‰) å› ä¸º teacher_raw æ­¤æ—¶è¿˜æ²¡ wrap
        state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}
        msg = teacher_raw.load_state_dict(state_dict, strict=False)
        print(f"   Teacher loaded: {msg}")
    else:
        print(f"âŒ Error: Teacher path not found {args.teacher_path}")
        return

    teacher_raw.eval()
    for p in teacher_raw.parameters(): p.requires_grad = False
    
    # ğŸŒŸ ä½¿ç”¨ FeatureWrapper åŒ…è£… Teacher
    teacher = FeatureWrapper(teacher_raw)

    # Student
    student_raw = get_model(args.student, num_classes=100).to(device)
    # ğŸŒŸ ä½¿ç”¨ FeatureWrapper åŒ…è£… Student
    student = FeatureWrapper(student_raw)

    # --- 3. ä¼˜åŒ–å™¨ ---
    optimizer = optim.SGD(student.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)
    scheduler = get_lr_scheduler(optimizer, args.epochs, warmup_epochs=5)

    # --- 4. æ—¥å¿— ---
    if not os.path.exists(args.save_dir): os.makedirs(args.save_dir)
    log_path = os.path.join(args.save_dir, f'{args.student}_{args.teacher}_distill_improved_log.csv')
    with open(log_path, 'w', newline='') as f:
        csv.writer(f).writerow(['epoch', 'loss', 'loss_ce', 'loss_js', 'loss_kd', 'loss_rkd', 'clean_acc', 'time'])

    best_acc = 0.0

    # --- 5. è®­ç»ƒå¾ªç¯ ---
    for epoch in range(args.epochs):
        student.train()
        total_loss = 0.0
        start_time = time.time()
        
        # è®°å½•åˆ†é¡¹ Loss æ–¹ä¾¿ debug
        m_ce, m_js, m_kd, m_rkd = 0.0, 0.0, 0.0, 0.0
        
        for i, (images_clean, images_aug1, images_aug2, targets) in enumerate(trainloader):
            images_clean, images_aug1, images_aug2, targets = \
                images_clean.to(device), images_aug1.to(device), images_aug2.to(device), targets.to(device)
            
            # æ‹¼æ¥
            images_all = torch.cat([images_clean, images_aug1, images_aug2], dim=0)

            optimizer.zero_grad()

            # --- Forward ---
            # Student: è·å– logits å’Œ features
            logits_all_s, feats_all_s = student(images_all)
            logits_clean_s, logits_aug1_s, logits_aug2_s = torch.split(logits_all_s, images_clean.size(0))
            # åªå– Clean æ•°æ®çš„ç‰¹å¾åš RKD (é¿å… AugMix çš„å¼ºæ‰­æ›²ç ´åæµå½¢ç»“æ„)
            feats_clean_s, _, _ = torch.split(feats_all_s, images_clean.size(0))

            # Teacher: Forward (No Grad)
            with torch.no_grad():
                logits_all_t, feats_all_t = teacher(images_all)
                feats_clean_t, _, _ = torch.split(feats_all_t, images_clean.size(0))

            # --- Loss Calculation ---

            # 1. CE Loss (Clean Classification)
            loss_ce = F.cross_entropy(logits_clean_s, targets)

            # 2. AugMix JS Consistency
            p_clean = F.softmax(logits_clean_s, dim=1)
            p_aug1 = F.softmax(logits_aug1_s, dim=1)
            p_aug2 = F.softmax(logits_aug2_s, dim=1)
            p_mixture = torch.clamp((p_clean + p_aug1 + p_aug2) / 3., 1e-7, 1).log()
            loss_js = (F.kl_div(p_mixture, p_clean, reduction='batchmean') +
                       F.kl_div(p_mixture, p_aug1, reduction='batchmean') +
                       F.kl_div(p_mixture, p_aug2, reduction='batchmean')) / 3.

            # 3. Weighted KD Loss (Clean + Aug)
            # ä½¿ç”¨æ”¹è¿›çš„ "Confidence-Aware" KD
            loss_kd_val = confidence_weighted_kd_loss(logits_all_s, logits_all_t, args.temperature)

            # 4. RKD Loss (Clean Structure)
            # æ”¹è¿›çš„ "Relational" è’¸é¦
            loss_rkd_val = rkd_loss(feats_clean_s, feats_clean_t)

            # --- Total Loss ---
            loss = loss_ce + \
                   args.js_lambda * loss_js + \
                   args.alpha * loss_kd_val + \
                   args.beta * loss_rkd_val

            loss.backward()
            optimizer.step()
            
            # Stats
            total_loss += loss.item()
            m_ce += loss_ce.item()
            m_js += loss_js.item()
            m_kd += loss_kd_val.item()
            m_rkd += loss_rkd_val.item()

        # Validation
        student.eval()
        correct = 0; total = 0
        with torch.no_grad():
            for inputs, targets in testloader:
                inputs, targets = inputs.to(device), targets.to(device)
                # æ³¨æ„ï¼šstudent ç°åœ¨æ˜¯ FeatureWrapperï¼Œè¾“å‡ºæ˜¯ tuple
                outputs, _ = student(inputs) 
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        acc = 100.*correct/total
        scheduler.step()

        epoch_time = time.time() - start_time
        avg_loss = total_loss / (i+1)
        
        # æ‰“å°è¯¦ç»† Loss ç»„æˆï¼Œæ–¹ä¾¿è§‚å¯Ÿå“ªä¸ª Loss åœ¨èµ·ä½œç”¨
        print(f"Epoch {epoch+1}/{args.epochs} | Time: {epoch_time:.0f}s | Acc: {acc:.2f}%")
        print(f"   Loss: {avg_loss:.3f} (CE:{m_ce/(i+1):.2f} JS:{m_js/(i+1):.2f} KD:{m_kd/(i+1):.2f} RKD:{m_rkd/(i+1):.4f})")
        
        with open(log_path, 'a', newline='') as f:
            csv.writer(f).writerow([epoch+1, avg_loss, m_ce/(i+1), m_js/(i+1), m_kd/(i+1), m_rkd/(i+1), acc, epoch_time])

        if acc > best_acc:
            best_acc = acc
            # ä¿å­˜æ—¶ï¼Œæˆ‘ä»¬ä¿å­˜ student.model çš„ state_dict
            # è¿™æ ·ä»¥ååŠ è½½å°±ä¸éœ€è¦ FeatureWrapper äº†ï¼Œå˜å›æ™®é€šçš„ NormalizedModel
            torch.save({'net': student.model.state_dict(), 'acc': acc}, 
                       os.path.join(args.save_dir, f'{args.student}_{args.teacher}_distill_improved_best.pth'))

    print(f"=== è®­ç»ƒå®Œæˆ. Best Clean Acc: {best_acc:.2f}% ===")

if __name__ == "__main__":
    train_distill_improved()
```